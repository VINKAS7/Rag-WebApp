# RAG WebApp

A full-stack Retrieval-Augmented Generation application. The frontend (React + TypeScript) provides a clean chat interface, collection management, and a prompt editor. The backend (FastAPI) ingests documents and URLs, builds a ChromaDB vector store, and uses an advanced RAG pipeline with conversational memory to generate responses using Ollama.

---

## âœ¨ Highlights

- **Advanced RAG Pipeline**: Combines context from uploaded documents and past conversation history using Reciprocal Rank Fusion (RRF) for highly relevant answers.
- **Flexible Data Ingestion**: Create vector collections by uploading files (PDF, DOCX, PPTX, XLSX, WAV, MP3, images (PNG, TIFF, JPEG, ...)) or scraping content directly from URLs.
- **Conversational Memory**: Each chat maintains its own context, allowing for meaningful follow-up questions.
- **Modern Chat UI**: Responsive interface with conversation history, model/collection selection, and real-time updates.
- **Prompt Template Editor**: Easily switch between a default prompt and custom-built templates. Save new templates with required placeholders.
- **Route Hydration**: Direct navigation to a conversation URL (`/conversation/{id}`) automatically loads its chat history and state.

---

## ğŸ›ï¸ Architecture

The application consists of a **React frontend** and a **FastAPI backend**.  
The backend's core is its advanced RAG pipeline.

- **Frontend: React + TypeScript + Vite**
  - State Management: Redux Toolkit
  - Styling: Tailwind CSS

- **Backend: FastAPI**
  - Ingestion: docling (document parsing), sentence-transformers (embeddings), httpx (URL scraping).
  - Vector Storage: ChromaDB (separate persistent stores for documents and conversational context).
  - Metadata Storage: TinyDB (conversation history and prompt templates).
  - LLM Inference: Ollama.

### ğŸ”„ How It Works

When a user sends a prompt, the backend executes a hybrid retrieval strategy:

1. **Dual Retrieval**:  
   - Queries the **Document Store** (ChromaDB for the selected collection).  
   - Queries the **Conversational Context** (ChromaDB for the current chat session).  

2. **Fusion & Ranking (RRF)**:  
   - Merges results from both retrievals and re-ranks using Reciprocal Rank Fusion.  

3. **Generation**:  
   - Injects top-ranked context into a prompt template.  
   - Sends the final prompt to an Ollama model for response generation.  

```
User â†’ Frontend (React) â†’ Backend (FastAPI)
  â†³ Query:  ChromaDB (Docs) + ChromaDB (Conv. Context) â†’ RRF â†’ Ollama â†’ Response
  â†³ Ingest: Files/URLs â†’ Chunk + Embed â†’ ChromaDB (Persistent)
  â†³ Persist: TinyDB (History, Templates)
```

---

## ğŸ“ Repository Structure

- **frontend/** â†’ React application (see `frontend/README.md` for UI docs).  
- **backend/** â†’ FastAPI application (see `backend/README.md` for API docs).  
- **collections/** â†’ Created at runtime; holds:  
  - Document vector store (`chromadb`)  
  - Conversational context (`context`)  
  - Per-conversation logs (`db`)  
- **backend/db/** â†’ Global TinyDB stores for:  
  - `conversations_history.json` (conversation metadata)  
  - `prompt_templates.json` (saved templates)  

---

## âš™ï¸ Prerequisites

- Node.js **v18+**
- Python **v3.10+**
- [Ollama](https://ollama.ai/) installed with desired models (e.g., `ollama pull llama3`)

---

## ğŸš€ Setup and Run

### 1. Backend

```bash
# Navigate to backend
cd backend

# Install dependencies
pip install -r requirements.txt

# Start FastAPI server
uvicorn main:app --reload --host 127.0.0.1 --port 3000
```

Backend runs at â†’ [http://localhost:3000](http://localhost:3000)

---

### 2. Frontend

```bash
# Navigate to frontend
cd frontend

# Install dependencies
npm install

# Start dev server
npm run dev
```

Frontend runs at â†’ [http://localhost:5173](http://localhost:5173)

---

## â–¶ï¸ Core Flows

### ğŸ”¹ Create a Collection
- Click **Upload** â†’ Select files or paste URLs â†’ Name the collection â†’ Submit.  
- Backend processes data â†’ Collection becomes available.

### ğŸ”¹ Start a Conversation
- Select **Model** + **Collection** â†’ Enter a prompt.  
- App navigates to unique conversation URL.

### ğŸ”¹ Continue a Conversation
- Context retrieved from documents + conversation history.  
- Follow-up questions remain contextual.

### ğŸ”¹ Manage Prompt Templates
- Open **Prompt Settings** â†’ Choose default/custom template.  
- Create/edit templates in the editor (`{context}` and `{question}` required).

---

## ğŸ”Œ Backend API Summary

### Collections
- `GET /api/get_ollama_models` â†’ List available Ollama models.  
- `GET /api/get_collections` â†’ List all collections.  
- `POST /api/create_collection/{collection_name}` â†’ Ingest files/URLs.  

### Conversation
- `POST /conversation/get_response` â†’ Get a RAG-powered response.  
- `GET /conversation/get_conversation/{uid}` â†’ Get a conversationâ€™s history.  
- `GET /conversation/get_history` â†’ Get all conversations.  
- `DELETE /conversation/delete_conversation/{uid}` â†’ Delete a conversation.  

### Prompt Templates
- `GET /conversation/get_all_prompt_templates`  
- `POST /conversation/new_prompt_template`  
- `DELETE /conversation/delete_prompt_template/{template_name}`  
- `POST /conversation/use_default_prompt`  
- `GET /conversation/get_active_prompt_mode`  

(See `backend/README.md` for detailed request/response examples.)

---

## ğŸ’¾ Storage Layout

- `collections/{collectionName}/chromadb` â†’ Vector store (documents).  
- `collections/{collectionName}/context` â†’ Vector store (conversational memory).  
- `collections/{collectionName}/db/{conversation_id}.json` â†’ TinyDB log (per conversation).  
- `backend/db/conversations_history.json` â†’ Global conversation metadata.  
- `backend/db/prompt_templates.json` â†’ Global saved templates.  